{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20500 unique words in our dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load and treat vocab\n",
    "with open('vocab.txt') as file:\n",
    "    vocab = file.readlines()\n",
    "    vocab = [line.rstrip() for line in vocab]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('There are %d unique words in our dataset.' % vocab_size)\n",
    "\n",
    "# Hash table for words to indices and viceversa\n",
    "word_to_ix = { w:i for i,w in enumerate(vocab) }\n",
    "ix_to_word = { i:w for i,w in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '00',\n",
       " '0000',\n",
       " '000000',\n",
       " '001',\n",
       " '004',\n",
       " '008',\n",
       " '01',\n",
       " '010',\n",
       " '01087',\n",
       " '011',\n",
       " '012',\n",
       " '016',\n",
       " '01745',\n",
       " '01918',\n",
       " '02',\n",
       " '020',\n",
       " '0201',\n",
       " '0208',\n",
       " '03',\n",
       " '032',\n",
       " '04',\n",
       " '040',\n",
       " '05',\n",
       " '052',\n",
       " '055',\n",
       " '056',\n",
       " '057',\n",
       " '06',\n",
       " '07',\n",
       " '0710',\n",
       " '0725',\n",
       " '08',\n",
       " '080',\n",
       " '081',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '10006',\n",
       " '10007',\n",
       " '10029',\n",
       " '10079',\n",
       " '10081',\n",
       " '101',\n",
       " '1010',\n",
       " '102',\n",
       " '1024',\n",
       " '103',\n",
       " '1035',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '1082',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '111',\n",
       " '1110',\n",
       " '112',\n",
       " '1123',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '1179',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '1250',\n",
       " '1251',\n",
       " '1252',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '1282',\n",
       " '1284',\n",
       " '1285',\n",
       " '1288',\n",
       " '1291',\n",
       " '1296',\n",
       " '13',\n",
       " '1301',\n",
       " '1304',\n",
       " '1305',\n",
       " '1307',\n",
       " '131',\n",
       " '1311',\n",
       " '1314',\n",
       " '1317',\n",
       " '132',\n",
       " '1320',\n",
       " '133',\n",
       " '134',\n",
       " '137',\n",
       " '1370',\n",
       " '1377',\n",
       " '1394',\n",
       " '1395',\n",
       " '14',\n",
       " '141',\n",
       " '142',\n",
       " '1423',\n",
       " '14443',\n",
       " '145',\n",
       " '148',\n",
       " '1496',\n",
       " '15',\n",
       " '1500',\n",
       " '152',\n",
       " '1533',\n",
       " '1535',\n",
       " '156',\n",
       " '1578',\n",
       " '159',\n",
       " '1592',\n",
       " '16',\n",
       " '160',\n",
       " '162',\n",
       " '165',\n",
       " '1667',\n",
       " '1669',\n",
       " '1671',\n",
       " '1673',\n",
       " '1675',\n",
       " '168',\n",
       " '1680',\n",
       " '1682',\n",
       " '1687',\n",
       " '17',\n",
       " '171',\n",
       " '1713',\n",
       " '1716',\n",
       " '1732',\n",
       " '174',\n",
       " '175',\n",
       " '18',\n",
       " '180',\n",
       " '18030',\n",
       " '182',\n",
       " '1848',\n",
       " '185',\n",
       " '186',\n",
       " '1876',\n",
       " '19',\n",
       " '1918',\n",
       " '192',\n",
       " '1923',\n",
       " '1939',\n",
       " '1977',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2005',\n",
       " '2009',\n",
       " '2012',\n",
       " '2022',\n",
       " '2047',\n",
       " '2048',\n",
       " '2062',\n",
       " '2068',\n",
       " '20860',\n",
       " '21',\n",
       " '210',\n",
       " '2104',\n",
       " '2112',\n",
       " '2113',\n",
       " '2120',\n",
       " '2121',\n",
       " '213',\n",
       " '219',\n",
       " '22',\n",
       " '220',\n",
       " '2208',\n",
       " '2231',\n",
       " '2232',\n",
       " '224',\n",
       " '2250',\n",
       " '23',\n",
       " '2315',\n",
       " '232',\n",
       " '234',\n",
       " '2345',\n",
       " '2388',\n",
       " '24',\n",
       " '2400',\n",
       " '2406',\n",
       " '2426',\n",
       " '2438',\n",
       " '2445',\n",
       " '2480',\n",
       " '2483',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '251',\n",
       " '2523',\n",
       " '253',\n",
       " '255',\n",
       " '25519',\n",
       " '25538419688844',\n",
       " '256',\n",
       " '26',\n",
       " '263',\n",
       " '264',\n",
       " '27',\n",
       " '270',\n",
       " '2706',\n",
       " '2709',\n",
       " '28',\n",
       " '2800',\n",
       " '2821',\n",
       " '2822',\n",
       " '2834',\n",
       " '2864',\n",
       " '29',\n",
       " '30',\n",
       " '3000',\n",
       " '3068',\n",
       " '3072',\n",
       " '31',\n",
       " '3110',\n",
       " '3119',\n",
       " '3164',\n",
       " '32',\n",
       " '320',\n",
       " '3270',\n",
       " '33',\n",
       " '332',\n",
       " '3339',\n",
       " '3365',\n",
       " '34',\n",
       " '341',\n",
       " '3410',\n",
       " '3424',\n",
       " '345',\n",
       " '3455',\n",
       " '3484',\n",
       " '35',\n",
       " '350',\n",
       " '3526',\n",
       " '36',\n",
       " '360',\n",
       " '3657',\n",
       " '37',\n",
       " '38',\n",
       " '384',\n",
       " '386',\n",
       " '39',\n",
       " '390',\n",
       " '3950',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '4014',\n",
       " '405',\n",
       " '406',\n",
       " '4096',\n",
       " '41',\n",
       " '416',\n",
       " '42',\n",
       " '420',\n",
       " '422',\n",
       " '425',\n",
       " '43',\n",
       " '437',\n",
       " '44',\n",
       " '443',\n",
       " '444',\n",
       " '45',\n",
       " '4510',\n",
       " '456',\n",
       " '46',\n",
       " '47',\n",
       " '4716',\n",
       " '477',\n",
       " '48',\n",
       " '485',\n",
       " '4897810065911',\n",
       " '49',\n",
       " '496',\n",
       " '50',\n",
       " '500',\n",
       " '501',\n",
       " '509',\n",
       " '51',\n",
       " '512',\n",
       " '52',\n",
       " '520',\n",
       " '521',\n",
       " '522022',\n",
       " '53',\n",
       " '530',\n",
       " '532',\n",
       " '5351',\n",
       " '54',\n",
       " '540',\n",
       " '5424',\n",
       " '55',\n",
       " '5511',\n",
       " '553',\n",
       " '5530',\n",
       " '5536',\n",
       " '555',\n",
       " '559',\n",
       " '56',\n",
       " '5600',\n",
       " '561',\n",
       " '562',\n",
       " '5634',\n",
       " '565',\n",
       " '568',\n",
       " '57',\n",
       " '573',\n",
       " '578',\n",
       " '58',\n",
       " '581',\n",
       " '584',\n",
       " '586',\n",
       " '588',\n",
       " '59',\n",
       " '591',\n",
       " '594',\n",
       " '596',\n",
       " '597',\n",
       " '60',\n",
       " '600',\n",
       " '601',\n",
       " '604',\n",
       " '607',\n",
       " '61',\n",
       " '610',\n",
       " '6100',\n",
       " '6103',\n",
       " '6105',\n",
       " '615',\n",
       " '61883',\n",
       " '62',\n",
       " '620',\n",
       " '623',\n",
       " '626',\n",
       " '627724',\n",
       " '63',\n",
       " '630',\n",
       " '633',\n",
       " '639',\n",
       " '64',\n",
       " '640',\n",
       " '6429',\n",
       " '6455',\n",
       " '65',\n",
       " '6502',\n",
       " '6513',\n",
       " '653052',\n",
       " '66',\n",
       " '6612',\n",
       " '67',\n",
       " '679509',\n",
       " '68',\n",
       " '6800',\n",
       " '686',\n",
       " '69',\n",
       " '70',\n",
       " '707',\n",
       " '71',\n",
       " '711',\n",
       " '72',\n",
       " '721',\n",
       " '722',\n",
       " '723',\n",
       " '73',\n",
       " '731',\n",
       " '732',\n",
       " '733',\n",
       " '737',\n",
       " '74',\n",
       " '75',\n",
       " '7507',\n",
       " '76',\n",
       " '77',\n",
       " '7727',\n",
       " '775',\n",
       " '7750',\n",
       " '7751',\n",
       " '78',\n",
       " '7816',\n",
       " '784',\n",
       " '787',\n",
       " '79',\n",
       " '80',\n",
       " '800',\n",
       " '802',\n",
       " '8021',\n",
       " '80211',\n",
       " '8023',\n",
       " '804',\n",
       " '8051',\n",
       " '80510',\n",
       " '805100',\n",
       " '805101',\n",
       " '805102',\n",
       " '805103',\n",
       " '805104',\n",
       " '805105',\n",
       " '805106',\n",
       " '805107',\n",
       " '805108',\n",
       " '805109',\n",
       " '80511',\n",
       " '805110',\n",
       " '805111',\n",
       " '805112',\n",
       " '805113',\n",
       " '805114',\n",
       " '805115',\n",
       " '805116',\n",
       " '805117',\n",
       " '805118',\n",
       " '805119',\n",
       " '80512',\n",
       " '805120',\n",
       " '805121',\n",
       " '805122',\n",
       " '805123',\n",
       " '805124',\n",
       " '805125',\n",
       " '805126',\n",
       " '805127',\n",
       " '805128',\n",
       " '805129',\n",
       " '80513',\n",
       " '805130',\n",
       " '805131',\n",
       " '805132',\n",
       " '805133',\n",
       " '805134',\n",
       " '805135',\n",
       " '805136',\n",
       " '805137',\n",
       " '805138',\n",
       " '805139',\n",
       " '80514',\n",
       " '805140',\n",
       " '805141',\n",
       " '805142',\n",
       " '805143',\n",
       " '805144',\n",
       " '805145',\n",
       " '805146',\n",
       " '805147',\n",
       " '805148',\n",
       " '805149',\n",
       " '80515',\n",
       " '805150',\n",
       " '805151',\n",
       " '805152',\n",
       " '805153',\n",
       " '805154',\n",
       " '805155',\n",
       " '805156',\n",
       " '805157',\n",
       " '805158',\n",
       " '805159',\n",
       " '80516',\n",
       " '805160',\n",
       " '805161',\n",
       " '805162',\n",
       " '805163',\n",
       " '805164',\n",
       " '805165',\n",
       " '805166',\n",
       " '805167',\n",
       " '805168',\n",
       " '805169',\n",
       " '80517',\n",
       " '805170',\n",
       " '805171',\n",
       " '805172',\n",
       " '805173',\n",
       " '805174',\n",
       " '805175',\n",
       " '805176',\n",
       " '805177',\n",
       " '805178',\n",
       " '805179',\n",
       " '80518',\n",
       " '805180',\n",
       " '805181',\n",
       " '805182',\n",
       " '805183',\n",
       " '805184',\n",
       " '805185',\n",
       " '805186',\n",
       " '805187',\n",
       " '805188',\n",
       " '805189',\n",
       " '80519',\n",
       " '805190',\n",
       " '805191',\n",
       " '805192',\n",
       " '805193',\n",
       " '805194',\n",
       " '805195',\n",
       " '805196',\n",
       " '805197',\n",
       " '805198',\n",
       " '805199',\n",
       " '81',\n",
       " '810',\n",
       " '815',\n",
       " '82',\n",
       " '820',\n",
       " '8212',\n",
       " '822',\n",
       " '823',\n",
       " '8237',\n",
       " '8252',\n",
       " '8253',\n",
       " '82802',\n",
       " '83',\n",
       " '830',\n",
       " '8300',\n",
       " '836',\n",
       " '83627',\n",
       " '83697',\n",
       " '83843',\n",
       " '83877',\n",
       " '83977',\n",
       " '84',\n",
       " '840',\n",
       " '85',\n",
       " '850',\n",
       " '8500',\n",
       " '852',\n",
       " '855',\n",
       " '857',\n",
       " '86',\n",
       " '860',\n",
       " '8601',\n",
       " '861',\n",
       " '8610',\n",
       " '862',\n",
       " '863',\n",
       " '864',\n",
       " '865',\n",
       " '866',\n",
       " '868',\n",
       " '869',\n",
       " '87',\n",
       " '8703',\n",
       " '8705',\n",
       " '8707',\n",
       " '8712',\n",
       " '8716',\n",
       " '8718',\n",
       " '8736',\n",
       " '874',\n",
       " '88',\n",
       " '8859',\n",
       " '89',\n",
       " '90',\n",
       " '900',\n",
       " '91',\n",
       " '9101',\n",
       " '915',\n",
       " '92',\n",
       " '923',\n",
       " '93',\n",
       " '931',\n",
       " '94',\n",
       " '945',\n",
       " '95',\n",
       " '958',\n",
       " '959',\n",
       " '96',\n",
       " '963',\n",
       " '965',\n",
       " '9660',\n",
       " '97',\n",
       " '975',\n",
       " '98',\n",
       " '981',\n",
       " '986',\n",
       " '99',\n",
       " '999',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aac',\n",
       " 'aacdr',\n",
       " 'aacenc',\n",
       " 'aafs',\n",
       " 'aai',\n",
       " 'aal',\n",
       " 'aaxine',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abar',\n",
       " 'abbr',\n",
       " 'abbrev',\n",
       " 'abbreviate',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abbrevs',\n",
       " 'abc',\n",
       " 'abcd',\n",
       " 'abck',\n",
       " 'abe',\n",
       " 'abgr',\n",
       " 'abi',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'abl',\n",
       " 'able',\n",
       " 'ablt',\n",
       " 'abm',\n",
       " 'abnormal',\n",
       " 'abon',\n",
       " 'abook',\n",
       " 'abor',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abox',\n",
       " 'abr',\n",
       " 'abrt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absem',\n",
       " 'absent',\n",
       " 'abser',\n",
       " 'absolute',\n",
       " 'absorb',\n",
       " 'absorbing',\n",
       " 'abst',\n",
       " 'absthr',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'absu',\n",
       " 'abut',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'acar',\n",
       " 'acasecmp',\n",
       " 'acav',\n",
       " 'acc',\n",
       " 'acca',\n",
       " 'acccon',\n",
       " 'accel',\n",
       " 'accelerate',\n",
       " 'accelerator',\n",
       " 'accelerators',\n",
       " 'accelerometer',\n",
       " 'accels',\n",
       " 'accent',\n",
       " 'accenting',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accesses',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessions',\n",
       " 'acci',\n",
       " 'accidental',\n",
       " 'acclst',\n",
       " 'accm',\n",
       " 'accommodate',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'acct',\n",
       " 'accu',\n",
       " 'accum',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulator',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'ace',\n",
       " 'acep',\n",
       " 'acer',\n",
       " 'acfc',\n",
       " 'acg',\n",
       " 'ach',\n",
       " 'ache',\n",
       " 'acht',\n",
       " 'achtbc',\n",
       " 'achtbl',\n",
       " 'achtbs',\n",
       " 'achtcl',\n",
       " 'achtcs',\n",
       " 'achtds',\n",
       " 'achtil',\n",
       " 'achtl',\n",
       " 'achtlb',\n",
       " 'achtlc',\n",
       " 'achtld',\n",
       " 'achtli',\n",
       " 'achtll',\n",
       " 'achtlr',\n",
       " 'achtlu',\n",
       " 'achtlx',\n",
       " 'achtrl',\n",
       " 'achtrs',\n",
       " 'achtul',\n",
       " 'achtus',\n",
       " 'achtxl',\n",
       " 'achtxs',\n",
       " 'acim',\n",
       " 'ack',\n",
       " 'ackci',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acl',\n",
       " 'aclose',\n",
       " 'aclri',\n",
       " 'aclrs',\n",
       " 'aclt',\n",
       " 'acm',\n",
       " 'acmd',\n",
       " 'acmod',\n",
       " 'acolyte',\n",
       " 'acpi',\n",
       " 'acpwr',\n",
       " 'acq',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquisition',\n",
       " 'acr',\n",
       " 'acre',\n",
       " 'acron',\n",
       " 'acrons',\n",
       " 'acs',\n",
       " 'acsc',\n",
       " 'acssdr',\n",
       " 'act',\n",
       " 'actf',\n",
       " 'actfrac',\n",
       " 'acti',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actioner',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'activator',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actl',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actrg',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actualize',\n",
       " 'actually',\n",
       " 'acw',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adapted',\n",
       " 'adapter',\n",
       " 'adapters',\n",
       " 'adaptive',\n",
       " 'adaptor',\n",
       " 'adaptors',\n",
       " 'adate',\n",
       " 'adc',\n",
       " 'adctest',\n",
       " 'add',\n",
       " 'adda',\n",
       " 'added',\n",
       " 'addend',\n",
       " 'addenda',\n",
       " 'addends',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'addr',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adef',\n",
       " 'adeh',\n",
       " 'adf',\n",
       " 'adios',\n",
       " 'adj',\n",
       " 'adjacent',\n",
       " 'adjday',\n",
       " 'adje',\n",
       " 'adjl',\n",
       " 'adjlon',\n",
       " 'adjmin',\n",
       " 'adjmon',\n",
       " 'adjoin',\n",
       " 'adjourn',\n",
       " 'adjpar',\n",
       " 'adjtitl',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjuster',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adjvol',\n",
       " 'adjwday',\n",
       " 'adkim',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'admit',\n",
       " 'admonish',\n",
       " 'admtek',\n",
       " 'adn',\n",
       " 'adns',\n",
       " 'ado',\n",
       " 'adobe',\n",
       " 'adoc',\n",
       " 'adopt',\n",
       " 'adoptions',\n",
       " 'adorn',\n",
       " 'adp',\n",
       " 'adpa',\n",
       " 'adpcm',\n",
       " 'adr',\n",
       " 'adrof',\n",
       " 'adrs',\n",
       " 'ads',\n",
       " 'adt',\n",
       " 'adu',\n",
       " 'adup',\n",
       " 'adus',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advc',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advertisement',\n",
       " 'advertising',\n",
       " 'advise',\n",
       " 'advt',\n",
       " 'advws',\n",
       " 'adx',\n",
       " 'ae',\n",
       " 'aead',\n",
       " 'aend',\n",
       " 'aeol',\n",
       " 'aep',\n",
       " 'aer',\n",
       " 'aes',\n",
       " 'af',\n",
       " 'afc',\n",
       " 'afe',\n",
       " 'aff',\n",
       " 'affected',\n",
       " 'affine',\n",
       " 'affinities',\n",
       " 'affinity',\n",
       " 'affix',\n",
       " 'afford',\n",
       " 'afh',\n",
       " 'afield',\n",
       " 'afile',\n",
       " 'aflag',\n",
       " 'afm',\n",
       " 'afmt',\n",
       " 'afor',\n",
       " 'afp',\n",
       " 'afprun',\n",
       " 'afree',\n",
       " 'afs',\n",
       " 'afsk',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'aftermath',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agar',\n",
       " 'agate',\n",
       " 'agc',\n",
       " 'agcep',\n",
       " 'agde',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ageing',\n",
       " 'agemap',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'aget',\n",
       " 'agf',\n",
       " 'agfst',\n",
       " 'agfstin',\n",
       " 'agg',\n",
       " 'agget',\n",
       " 'aggr',\n",
       " 'aggravate',\n",
       " 'aggregate',\n",
       " 'aggregation',\n",
       " 'aggregator',\n",
       " 'aglen',\n",
       " 'agn',\n",
       " 'agnx',\n",
       " 'agnxt',\n",
       " 'agnxtin',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'ags',\n",
       " 'agset',\n",
       " 'agt',\n",
       " 'ague',\n",
       " 'agxb',\n",
       " 'agxbput',\n",
       " 'agxget',\n",
       " 'agxset',\n",
       " 'ah',\n",
       " 'ahci',\n",
       " 'ahead',\n",
       " 'ai',\n",
       " 'aible',\n",
       " 'aic',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aif',\n",
       " 'aiff',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'ainit',\n",
       " 'air',\n",
       " 'airbrush',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'aisle',\n",
       " 'aix',\n",
       " 'aje',\n",
       " 'ajour',\n",
       " 'ajoute',\n",
       " 'ajp',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akai',\n",
       " 'akey',\n",
       " 'akm',\n",
       " 'akmp',\n",
       " 'al',\n",
       " 'alac',\n",
       " 'alaf',\n",
       " 'alai',\n",
       " 'alarm',\n",
       " 'alarming',\n",
       " 'alarms',\n",
       " 'alaw',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alc',\n",
       " 'aldap',\n",
       " 'alemu',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'alfred',\n",
       " 'alg',\n",
       " 'algebraic',\n",
       " 'algo',\n",
       " 'algor',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'algs',\n",
       " 'ali',\n",
       " 'alias',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set. \n",
    "dfs = pd.read_csv('ordered-labels-dataset-subsample.txt', delim_whitespace=True, index_col = 0, header = None, engine='python') #keep_default_na = False, Keep 'null' as valid value.\n",
    "\n",
    "# Build set with lists\n",
    "dfs['Words'] = dfs.dropna()[2].apply(lambda x: x.split('_'))\n",
    "# Build input for transformer: add space and end of line\n",
    "dfs['Names'] = dfs.dropna()['Words'].apply(lambda x: str(' ' + ' '.join(x) + ' \\n'))\n",
    "words_lists = list(dfs.loc[:,'Words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = dfs['Names'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "32977             dpkg set prog name \\n\n",
       "32978              get int attribute \\n\n",
       "32979                  finalize test \\n\n",
       "32980                  run psm shell \\n\n",
       "32981      erlang backings tore grow \\n\n",
       "                      ...              \n",
       "329784                         sr dt \\n\n",
       "329785             chirp client link \\n\n",
       "329786           cgr preview forward \\n\n",
       "329787            game set color num \\n\n",
       "329788            mdns ec dns packet \\n\n",
       "Name: Names, Length: 296744, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.to_csv('names.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    296744.000000\n",
       "mean          3.656994\n",
       "std           1.577809\n",
       "min           1.000000\n",
       "25%           3.000000\n",
       "50%           3.000000\n",
       "75%           5.000000\n",
       "max          17.000000\n",
       "Name: Name_length, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats regarding len of names\n",
    "dfs['Name_length'] = dfs['Words'].dropna().apply(lambda x: len(x))\n",
    "\n",
    "dfs['Name_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test. Build probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test set\n",
    "X_train, X_test = train_test_split(words_lists, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of names: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  8,  0, ...,  1,  1,  0],\n",
       "       [ 0, 53,  0, ...,  0,  0,  1],\n",
       "       [ 0,  0,  1, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0, 68,  2, ...,  1,  1,  2]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of positions observed\n",
    "Tx = int(max(dfs['Name_length']))\n",
    "print('Maximum length of names:', Tx)\n",
    "\n",
    "# Matrix counting word appearances, row for position, columns for each word (among 20500 words)\n",
    "word_freq = np.zeros((Tx+1,vocab_size), dtype=np.int32)\n",
    "\n",
    "for i in range(Tx):\n",
    "    for word_list in X_train:\n",
    "        if isinstance(word_list,list) and len(word_list) > i:\n",
    "            word_freq[i,word_to_ix[word_list[i]] ] += 1\n",
    "\n",
    "# Last row: sum of appearances of each word\n",
    "for j in range(vocab_size):\n",
    "    word_freq[Tx,j] = sum(word_freq[:,j])\n",
    "            \n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words not appearing in training set:\n",
    "sum(word_freq[Tx,:] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_normalized = word_freq[:Tx,:].astype(np.float32)\n",
    "\n",
    "# Normalize\n",
    "for j in range(vocab_size):\n",
    "    if sum(word_freq_normalized[:,j]) != 0:\n",
    "        word_freq_normalized[:,j] /= sum(word_freq_normalized[:Tx,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 20500)\n",
      "[[ 0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 53  0  0  9  0  0  3  0  0  0  2  0  0  0  1  0  3  0  0]\n",
      " [ 0  0  1  2  0  1  0  2  0  0  0  0  0  1  1  5  0  0  0  1]\n",
      " [ 0  4  1  1  0  0  1  4  2  0  1  0  0  0  0 16  1  0  1  1]\n",
      " [ 0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 68  2  3  9  1  1 10  4  0  1  2  0  1  1 23  1  3  1  3]]\n"
     ]
    }
   ],
   "source": [
    "# Summary of obtained matrices\n",
    "print(np.shape(word_freq))\n",
    "print(word_freq[:18,:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 20500)\n",
      "[[0.         0.11764706 0.         0.         0.        ]\n",
      " [0.         0.7794118  0.         0.         1.        ]\n",
      " [0.         0.         0.5        0.6666667  0.        ]\n",
      " [0.         0.05882353 0.5        0.33333334 0.        ]\n",
      " [0.         0.02941176 0.         0.         0.        ]\n",
      " [0.         0.01470588 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word_freq_normalized))\n",
    "print(word_freq_normalized[:18,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot showing names' length\n",
    "plt.plot([sum(word_freq[x,:]) for x in range(17)])\n",
    "plt.title('Names\\' lengths')\n",
    "plt.ylabel('Number of names')\n",
    "plt.xlabel('Number of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_probabilities(word_list):\n",
    "    permutations = list(itertools.permutations(word_list, len(word_list)))\n",
    "    print(permutations)\n",
    "    probs = np.zeros(len(permutations))\n",
    "    for ix, permutation in enumerate(permutations):\n",
    "        prob_tmp = 1\n",
    "        for i in range(len(permutation)):\n",
    "            prob_tmp *= word_freq_normalized[ i, word_to_ix[permutation[i]] ]\n",
    "            print(word_freq_normalized[ :, word_to_ix[permutation[i]] ])\n",
    "        probs[ix] = prob_tmp\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gdalget', 'gdu'), ('gdu', 'gdalget')]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.9655172  0.03448276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "[0.9655172  0.03448276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03448276, 0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtain_probabilities(['gdalget','gdu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend GPT2 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful links for treating vocab:\n",
    "# https://discuss.huggingface.co/t/roberta-from-scratch-with-different-vocab-vs-fine-tuning/569\n",
    "# https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose DistilGPT2: GPT2 lighter version\n",
    "model_checkpoint = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) #vocab_file = 'vocab.txt' ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should I use the following vocab set ?\n",
    "# vocab_with_symbol = ['' + word for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CS': 9429,\n",
       " 'Thumbnails': 28924,\n",
       " 'ghan': 6064,\n",
       " 'porting': 26527,\n",
       " 'Latter': 34437,\n",
       " 'Lore': 15639,\n",
       " 'Territories': 42354,\n",
       " 'pertinent': 35268,\n",
       " 'technology': 3037,\n",
       " 'crus': 25164,\n",
       " 'calcium': 19700,\n",
       " 'majority': 35839,\n",
       " 'Eval': 26439,\n",
       " 'faux': 36748,\n",
       " 'agric': 8986,\n",
       " 'cook': 4255,\n",
       " 'definitive': 17347,\n",
       " 'Gallery': 29352,\n",
       " 'Enable': 36695,\n",
       " 'antidepressant': 41897,\n",
       " 'Europeans': 20006,\n",
       " 'repent': 28787,\n",
       " 'sweating': 38912,\n",
       " '################': 14468,\n",
       " 'smokers': 24109,\n",
       " 'unamb': 42053,\n",
       " 'oooo': 13321,\n",
       " 'Errors': 44225,\n",
       " 'Dresden': 46993,\n",
       " '================': 4770,\n",
       " 'Zip': 41729,\n",
       " 'acc': 697,\n",
       " 'lights': 8091,\n",
       " 'Probably': 34784,\n",
       " 'Un': 791,\n",
       " 'Observer': 27058,\n",
       " 'Preferences': 49780,\n",
       " 'psons': 31410,\n",
       " 'unborn': 36172,\n",
       " 'rain': 6290,\n",
       " 'motorcycles': 39404,\n",
       " 'ABV': 49993,\n",
       " 'akia': 21897,\n",
       " 'Pale': 21706,\n",
       " 'arsen': 38924,\n",
       " 'obil': 25898,\n",
       " 'contradicts': 40081,\n",
       " 'exemplary': 40690,\n",
       " 'non': 1729,\n",
       " 'except': 2845,\n",
       " 'hyper': 8718,\n",
       " 'fucking': 9372,\n",
       " 'ble': 903,\n",
       " 'Consider': 19626,\n",
       " '384': 22842,\n",
       " 'committed': 5364,\n",
       " 'sends': 12800,\n",
       " 'liqu': 14756,\n",
       " 'Simple': 17427,\n",
       " 'Schwar': 29726,\n",
       " 'stereotype': 31240,\n",
       " '\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': 34604,\n",
       " '885': 44230,\n",
       " 'paranoid': 30285,\n",
       " 'allowable': 49299,\n",
       " 'ivory': 32630,\n",
       " 'Eat': 27574,\n",
       " 'append': 24443,\n",
       " 'Bah': 13081,\n",
       " 'splits': 30778,\n",
       " 'Ire': 7181,\n",
       " 'wana': 49484,\n",
       " 'people': 661,\n",
       " 'Olympic': 11514,\n",
       " 'respiratory': 22949,\n",
       " 'aco': 10602,\n",
       " 'phenomen': 7581,\n",
       " 'diplomatic': 13093,\n",
       " 'aky': 15492,\n",
       " 'modifications': 19008,\n",
       " 'ping': 29400,\n",
       " 'Zam': 38343,\n",
       " 'renewed': 16434,\n",
       " 'Clarkson': 40524,\n",
       " 'Actor': 27274,\n",
       " 'SAY': 45687,\n",
       " 'Numbers': 49601,\n",
       " 'weather': 6193,\n",
       " 'YOU': 36981,\n",
       " 'punished': 16851,\n",
       " 'suing': 28941,\n",
       " 'Hard': 6912,\n",
       " 'clamation': 20931,\n",
       " '': 21356,\n",
       " '329': 42141,\n",
       " 'normal': 11265,\n",
       " 'substant': 5925,\n",
       " 'intervention': 9572,\n",
       " 'presumed': 25751,\n",
       " 'PORT': 15490,\n",
       " 'iliated': 31705,\n",
       " 'consumer': 49827,\n",
       " 'fort': 6285,\n",
       " 'unconventional': 34014,\n",
       " 'addons': 39996,\n",
       " 'criterion': 34054,\n",
       " 'cotton': 15985,\n",
       " 'swimming': 14899,\n",
       " 'illustrates': 21290,\n",
       " 'elegance': 49198,\n",
       " 'fue': 37911,\n",
       " 'Jugg': 39296,\n",
       " 'Previous': 21801,\n",
       " 'following': 1708,\n",
       " 'OPLE': 34354,\n",
       " 'drawer': 33451,\n",
       " 'tariff': 36427,\n",
       " 'Atlantic': 41120,\n",
       " 'Trave': 43662,\n",
       " 'Teaching': 38094,\n",
       " 'contem': 7887,\n",
       " 'quarry': 47780,\n",
       " '290': 24369,\n",
       " 'cy': 948,\n",
       " 'UR': 4261,\n",
       " 'Spit': 48377,\n",
       " 'chron': 16199,\n",
       " 'Watson': 14959,\n",
       " '520': 36141,\n",
       " 'solves': 39107,\n",
       " 'WikiLeaks': 19766,\n",
       " 'cliffe': 33783,\n",
       " 'Mechanics': 47570,\n",
       " 'esson': 39670,\n",
       " 'Ok': 18690,\n",
       " 'debating': 31174,\n",
       " 'Mart': 3981,\n",
       " 'Vox': 28035,\n",
       " 'unbiased': 46735,\n",
       " 'O': 440,\n",
       " 'percentage': 5873,\n",
       " 'tl': 28781,\n",
       " 'Joseph': 7212,\n",
       " 'Debian': 26062,\n",
       " 'Pork': 44062,\n",
       " 'Component': 35100,\n",
       " 'profess': 2992,\n",
       " 'bombard': 35285,\n",
       " 'glamorous': 46185,\n",
       " 'Hemp': 50060,\n",
       " 'omic': 10179,\n",
       " 'Jord': 38317,\n",
       " 'immortal': 26156,\n",
       " 'typical': 7226,\n",
       " '270': 20479,\n",
       " 'Ass': 8021,\n",
       " 'Studio': 11733,\n",
       " 'replication': 30330,\n",
       " 'clinical': 8668,\n",
       " 'bedroom': 36269,\n",
       " 'Insight': 39917,\n",
       " 'builds': 12188,\n",
       " 'acqu': 4078,\n",
       " 'strongest': 12841,\n",
       " 'certain': 1728,\n",
       " 'aer': 25534,\n",
       " 'ctrl': 44755,\n",
       " 'attacking': 9274,\n",
       " 'isition': 10027,\n",
       " 'obtained': 6492,\n",
       " 'Jong': 17960,\n",
       " '--------------------': 19351,\n",
       " 'assortment': 36168,\n",
       " 'essions': 6202,\n",
       " 'rehears': 28779,\n",
       " 'oration': 6944,\n",
       " 'aquin': 48734,\n",
       " 'refuel': 47874,\n",
       " 'andan': 42509,\n",
       " 'iop': 14922,\n",
       " 'empowerment': 39309,\n",
       " 'Rampage': 40244,\n",
       " 'undesirable': 38117,\n",
       " 'Chinatown': 47043,\n",
       " 'Bearing': 28834,\n",
       " 'disappro': 22293,\n",
       " 'KC': 25247,\n",
       " 'Bryant': 16754,\n",
       " 'saline': 47375,\n",
       " 'Vul': 25442,\n",
       " '571': 42875,\n",
       " 'verified': 19000,\n",
       " 'allegiance': 25696,\n",
       " 'utilized': 21487,\n",
       " 'onnaissance': 31539,\n",
       " 'rehearsal': 45795,\n",
       " 'ur': 2956,\n",
       " 'integ': 4132,\n",
       " 'ials': 8231,\n",
       " 'Anime': 27812,\n",
       " 'Gemini': 35495,\n",
       " 'Barrier': 32804,\n",
       " 'enfranch': 39827,\n",
       " 'perate': 30052,\n",
       " 'advertisers': 27835,\n",
       " 'nudity': 42156,\n",
       " 'Dating': 43528,\n",
       " 'anus': 47285,\n",
       " 'Swordsman': 48318,\n",
       " 'moistur': 41189,\n",
       " 'third': 17089,\n",
       " 'Bernie': 33433,\n",
       " 'triggers': 20022,\n",
       " 'Pearl': 18482,\n",
       " 'reward': 6721,\n",
       " 'Hank': 24386,\n",
       " 'Hive': 33235,\n",
       " 'research': 2267,\n",
       " '030': 39101,\n",
       " 'registrations': 47997,\n",
       " 'depleted': 34069,\n",
       " 'Closing': 47055,\n",
       " '134': 22352,\n",
       " 'puppet': 30095,\n",
       " '2015': 4626,\n",
       " 'itures': 20686,\n",
       " 'Justin': 33229,\n",
       " 'Cait': 34118,\n",
       " '490': 45601,\n",
       " 'Loading': 19031,\n",
       " 'Dig': 19511,\n",
       " 'agen': 11286,\n",
       " 'afety': 27925,\n",
       " 'companions': 19429,\n",
       " 'absent': 13717,\n",
       " '215': 22951,\n",
       " \"'?\": 30960,\n",
       " 'However': 4864,\n",
       " 'creepy': 23387,\n",
       " 'mom': 1995,\n",
       " 'Palm': 18358,\n",
       " 'Hercules': 32795,\n",
       " 'Purch': 34459,\n",
       " 'Rollins': 47489,\n",
       " '1945': 15761,\n",
       " 'specifying': 31577,\n",
       " 'belt': 10999,\n",
       " 'Ethernet': 31903,\n",
       " '403': 38210,\n",
       " 'avin': 20637,\n",
       " 'Dual': 36248,\n",
       " 'predic': 41219,\n",
       " 'PLoS': 49832,\n",
       " 'everyday': 10908,\n",
       " 'scoreboard': 50198,\n",
       " 'Next': 10019,\n",
       " 'doubts': 17188,\n",
       " 'olit': 6212,\n",
       " 'Sto': 22025,\n",
       " 'Centers': 22223,\n",
       " 'Blocks': 35111,\n",
       " 'expecting': 12451,\n",
       " 'pins': 20567,\n",
       " 'expenditures': 22895,\n",
       " 'Tracker': 35694,\n",
       " 'Dup': 37916,\n",
       " 'followers': 10569,\n",
       " '': 31732,\n",
       " 'Yo': 38101,\n",
       " 'leased': 40352,\n",
       " 'touch': 3638,\n",
       " 'Ped': 13457,\n",
       " '273': 38549,\n",
       " 'scaff': 41498,\n",
       " 'Twitter': 3009,\n",
       " 'advantages': 13391,\n",
       " 'Surve': 28095,\n",
       " 'sound': 2128,\n",
       " 'acknowled': 7333,\n",
       " 'cribed': 32968,\n",
       " 'bash': 41757,\n",
       " 'linebackers': 43081,\n",
       " 'TB': 22737,\n",
       " 'formulate': 46418,\n",
       " 'ksh': 50133,\n",
       " 'ARK': 14175,\n",
       " 'Entertainment': 11058,\n",
       " 'wants': 3382,\n",
       " 'Cheese': 27601,\n",
       " 'Rod': 6882,\n",
       " 'OS': 2640,\n",
       " 'hordes': 41872,\n",
       " 'Ads': 47442,\n",
       " 'wealth': 14298,\n",
       " 'swayed': 47632,\n",
       " 'Motorola': 31351,\n",
       " 'To': 2514,\n",
       " 'istrates': 37909,\n",
       " 'Ramsay': 47959,\n",
       " 'mamm': 13418,\n",
       " 'grows': 13676,\n",
       " 'interpret': 6179,\n",
       " 'WC': 28387,\n",
       " 'Got': 30074,\n",
       " 'actus': 34144,\n",
       " 'Music': 7849,\n",
       " 'gotten': 7891,\n",
       " 'stomach': 11384,\n",
       " 'module': 8265,\n",
       " 'BC': 11843,\n",
       " 'PLE': 16437,\n",
       " 'apesh': 25490,\n",
       " 'orrect': 47315,\n",
       " 'eks': 2573,\n",
       " 'histories': 25985,\n",
       " 'Slip': 49988,\n",
       " 'event': 1785,\n",
       " 'isms': 6583,\n",
       " 'Cool': 15226,\n",
       " 'Factory': 19239,\n",
       " 'elaide': 25078,\n",
       " 'Watch': 6305,\n",
       " 'alarmed': 32064,\n",
       " 'crim': 50086,\n",
       " 'donor': 17052,\n",
       " 'has': 468,\n",
       " 'tirelessly': 47905,\n",
       " 'stairs': 16046,\n",
       " 'eternal': 15851,\n",
       " 'catalog': 18388,\n",
       " 'ucci': 27501,\n",
       " 'Work': 5521,\n",
       " 'ADD': 27841,\n",
       " 'uted': 7241,\n",
       " 'Destiny': 17886,\n",
       " 'weapons': 33999,\n",
       " 'Recreation': 34285,\n",
       " 'denotes': 43397,\n",
       " 'spir': 9158,\n",
       " '97': 10111,\n",
       " 'JavaScript': 11933,\n",
       " 'imposing': 20814,\n",
       " 'strides': 35002,\n",
       " 'declines': 24459,\n",
       " 'Faust': 47411,\n",
       " 'punt': 35363,\n",
       " 'slot': 10852,\n",
       " 'Stud': 13007,\n",
       " '204': 26956,\n",
       " 'disruptive': 28094,\n",
       " 'Images': 29398,\n",
       " 'ague': 2064,\n",
       " 'Mines': 33466,\n",
       " 'unsatisf': 39264,\n",
       " 'publicized': 42732,\n",
       " 'engeance': 21364,\n",
       " 'philanthrop': 28150,\n",
       " 'STE': 24483,\n",
       " 'rak': 17716,\n",
       " '02': 7816,\n",
       " '](': 16151,\n",
       " 'AK': 10206,\n",
       " 'iker': 18320,\n",
       " 'ighton': 42993,\n",
       " 'HY': 43624,\n",
       " 'Ak': 33901,\n",
       " 'Badge': 44308,\n",
       " 'redited': 19465,\n",
       " 'sorcery': 47815,\n",
       " 'speak': 2740,\n",
       " 'misunder': 16782,\n",
       " 'trajectory': 22942,\n",
       " 'esm': 45798,\n",
       " 'DMV': 49887,\n",
       " ',)': 35751,\n",
       " 'outheast': 14474,\n",
       " '080': 33057,\n",
       " 'filming': 17691,\n",
       " 'encrypt': 34117,\n",
       " 'ir': 343,\n",
       " 'elli': 23225,\n",
       " 'Hero': 8757,\n",
       " 'uddenly': 18865,\n",
       " 'percentile': 37894,\n",
       " 'fleeting': 42738,\n",
       " 'Telecom': 44021,\n",
       " 'can': 460,\n",
       " 'thorough': 9321,\n",
       " 'icide': 5285,\n",
       " 'skillet': 41306,\n",
       " 'merce': 11647,\n",
       " '198': 2757,\n",
       " 'layout': 39786,\n",
       " 'Package': 15717,\n",
       " 'Ban': 30457,\n",
       " 'abling': 11716,\n",
       " 'Render': 46722,\n",
       " 'existent': 32786,\n",
       " 'iped': 46647,\n",
       " 'owl': 4883,\n",
       " 'met': 4164,\n",
       " 'oves': 5241,\n",
       " '': 15139,\n",
       " 'ceptive': 25867,\n",
       " 'REST': 30617,\n",
       " 'Appropriations': 37552,\n",
       " 'rapist': 38007,\n",
       " 'Huntington': 40644,\n",
       " 'afterward': 20875,\n",
       " 'LLP': 43245,\n",
       " 'dow': 47276,\n",
       " '169': 27191,\n",
       " 'puted': 17128,\n",
       " 'Mem': 4942,\n",
       " 'progresses': 33226,\n",
       " 'mu': 38779,\n",
       " 'onents': 3906,\n",
       " 'on': 261,\n",
       " 'spoon': 24556,\n",
       " 'IX': 10426,\n",
       " 'Consortium': 42727,\n",
       " 'ukong': 46654,\n",
       " '+++': 49954,\n",
       " 'shader': 33030,\n",
       " 'regulates': 39474,\n",
       " '1981': 14745,\n",
       " 'mathematical': 18069,\n",
       " 'Type': 6030,\n",
       " 'Item': 9097,\n",
       " 'Liberty': 14734,\n",
       " 'xxxxxxxx': 24223,\n",
       " 'preliminary': 15223,\n",
       " 'ears': 4127,\n",
       " '[[': 16410,\n",
       " 'highways': 27239,\n",
       " 'lot': 1256,\n",
       " 'whis': 12563,\n",
       " 'notations': 30078,\n",
       " 'aths': 33148,\n",
       " 'morals': 35472,\n",
       " 'oxic': 18047,\n",
       " 'Berk': 35595,\n",
       " 'reconnaissance': 39471,\n",
       " 'awan': 43004,\n",
       " '66': 7930,\n",
       " 'Arcane': 26475,\n",
       " 'ijah': 32778,\n",
       " 'yout': 32015,\n",
       " 'gements': 43547,\n",
       " 'Dodd': 32145,\n",
       " 'ctl': 34168,\n",
       " 'Chandra': 46295,\n",
       " 'tell': 1560,\n",
       " 'articles': 26845,\n",
       " '800': 7410,\n",
       " 'asers': 19865,\n",
       " 'occupancy': 42498,\n",
       " 'bay': 15489,\n",
       " 'teach': 4545,\n",
       " 'eff': 914,\n",
       " 'detection': 13326,\n",
       " 'eval': 18206,\n",
       " 'Ultimate': 47892,\n",
       " 'Plenty': 43257,\n",
       " 'udder': 41686,\n",
       " 'accomplishments': 26516,\n",
       " 'ual': 723,\n",
       " 'Rim': 29542,\n",
       " '+)': 28988,\n",
       " 'NW': 27605,\n",
       " 'though': 2016,\n",
       " 'Id': 7390,\n",
       " 'Improvements': 45097,\n",
       " 'Smith': 17919,\n",
       " 'KA': 25123,\n",
       " 'Skywalker': 29715,\n",
       " '513': 48645,\n",
       " 'transition': 6801,\n",
       " 'Rain': 31443,\n",
       " 'DOC': 37760,\n",
       " 'ilian': 35824,\n",
       " '258': 25600,\n",
       " 'applicable': 9723,\n",
       " 'Nay': 38808,\n",
       " 'mer': 4017,\n",
       " 'postal': 30793,\n",
       " '\"...': 26214,\n",
       " 'binary': 39491,\n",
       " 'ogi': 44381,\n",
       " 'Cubs': 21562,\n",
       " 'doping': 42631,\n",
       " 'pure': 5899,\n",
       " 'speed': 2866,\n",
       " 'women': 25878,\n",
       " 'appreciation': 19163,\n",
       " 'Andy': 35314,\n",
       " 'Peru': 25768,\n",
       " 'marg': 6145,\n",
       " 'Th': 536,\n",
       " 'International': 4037,\n",
       " 'Perform': 35006,\n",
       " '_': 4808,\n",
       " 'letal': 47293,\n",
       " 'justification': 17734,\n",
       " 'imedia': 20626,\n",
       " 'Timberwolves': 48983,\n",
       " 'Eng': 7936,\n",
       " 'Associ': 3928,\n",
       " 'Finished': 42931,\n",
       " 'detainee': 49436,\n",
       " 'repetition': 29693,\n",
       " 'Witch': 14522,\n",
       " 'priest': 11503,\n",
       " 'FA': 9677,\n",
       " 'daunting': 30496,\n",
       " 'Yanuk': 37068,\n",
       " 'ate': 15063,\n",
       " '((': 14808,\n",
       " 'state': 1181,\n",
       " 'drops': 10532,\n",
       " 'MLB': 18532,\n",
       " 'Tavern': 32693,\n",
       " 'RW': 33212,\n",
       " 'Household': 37306,\n",
       " 'rapes': 37459,\n",
       " 'RIP': 44967,\n",
       " 'caveats': 47155,\n",
       " 'empirical': 21594,\n",
       " 'Situation': 49465,\n",
       " 'Liang': 43322,\n",
       " 'api': 15042,\n",
       " 'Eye': 24876,\n",
       " 'Mont': 5575,\n",
       " 'lod': 19527,\n",
       " 'healthcare': 11409,\n",
       " 'embarrassing': 18997,\n",
       " 'hesis': 8497,\n",
       " 'mbol': 23650,\n",
       " 'comprehensive': 9815,\n",
       " 'sheets': 15747,\n",
       " 'relates': 18436,\n",
       " 'CHO': 49143,\n",
       " 'newcomers': 29661,\n",
       " 'Loader': 17401,\n",
       " 'ravings': 42335,\n",
       " 'Solution': 46344,\n",
       " 'RBI': 20948,\n",
       " 'grassroots': 23783,\n",
       " 'solder': 42809,\n",
       " 'Whitney': 34701,\n",
       " 'McN': 22586,\n",
       " 'ISC': 37719,\n",
       " 'azon': 5168,\n",
       " 'otechnology': 31201,\n",
       " 'bask': 39353,\n",
       " 'Cure': 36947,\n",
       " 'compound': 13061,\n",
       " 'appearances': 11057,\n",
       " 'alter': 8343,\n",
       " 'Belief': 49728,\n",
       " 'manufacturing': 9138,\n",
       " 'cedes': 19285,\n",
       " 'Chest': 25544,\n",
       " '313': 25838,\n",
       " 'sitting': 5586,\n",
       " 'reviewed': 32974,\n",
       " \"('\": 10786,\n",
       " '\":\"\",\"': 34713,\n",
       " 'violation': 8747,\n",
       " 'hangar': 44338,\n",
       " 'Verify': 49899,\n",
       " '860': 45039,\n",
       " 'rand': 25192,\n",
       " 'writing': 3597,\n",
       " 'Britain': 5491,\n",
       " 'NHS': 18183,\n",
       " 'ItemImage': 25502,\n",
       " 'Noah': 18394,\n",
       " 'puzzling': 42695,\n",
       " 'FIN': 20032,\n",
       " 'und': 3318,\n",
       " 'wrong': 36460,\n",
       " 'up': 929,\n",
       " 'ju': 14396,\n",
       " 'IDES': 42538,\n",
       " 'undergo': 17777,\n",
       " 'pattern': 3912,\n",
       " 'promoting': 11560,\n",
       " 'gs': 14542,\n",
       " 'bernatorial': 43660,\n",
       " 'landsl': 29433,\n",
       " 'Ottawa': 14074,\n",
       " 'Scouts': 30456,\n",
       " 'Uganda': 30872,\n",
       " 'bore': 18631,\n",
       " 'Cases': 35536,\n",
       " 'Were': 35653,\n",
       " '171': 28369,\n",
       " 'crocod': 37565,\n",
       " '755': 38172,\n",
       " 'ardy': 39124,\n",
       " 'Detroit': 40404,\n",
       " '060': 41322,\n",
       " 'Grammy': 42235,\n",
       " 'Trident': 47907,\n",
       " 'Jem': 48199,\n",
       " 'Links': 31815,\n",
       " 'defunct': 49119,\n",
       " '538': 49561,\n",
       " 'Dream': 7610,\n",
       " 'flips': 45971,\n",
       " 'Input': 23412,\n",
       " 'Position': 26545,\n",
       " 'ouston': 6526,\n",
       " 'dislike': 23109,\n",
       " 'pedal': 26667,\n",
       " '': 11976,\n",
       " 'rodu': 2076,\n",
       " 'sunlight': 19606,\n",
       " 'passports': 33052,\n",
       " 'certificate': 10703,\n",
       " 'Think': 22073,\n",
       " 'Astro': 35167,\n",
       " 'msec': 43242,\n",
       " '': 42637,\n",
       " 'Varg': 44684,\n",
       " 'itch': 49700,\n",
       " 'suffers': 21046,\n",
       " 'anuts': 37555,\n",
       " '1981': 35411,\n",
       " 'Representative': 19920,\n",
       " 'between': 23395,\n",
       " 'anecdotal': 41666,\n",
       " 'CON': 10943,\n",
       " 'Bugs': 44991,\n",
       " 'olia': 22703,\n",
       " 'ettings': 12374,\n",
       " 'olute': 3552,\n",
       " 'sy': 827,\n",
       " 'using': 3500,\n",
       " '.-': 7874,\n",
       " 'poster': 11968,\n",
       " 'science': 16801,\n",
       " 'LS': 30948,\n",
       " 'Mic': 25437,\n",
       " 'jured': 38608,\n",
       " 'activated': 13906,\n",
       " 'navy': 23956,\n",
       " 'depend': 4745,\n",
       " 'pillow': 28774,\n",
       " 'Robo': 39702,\n",
       " 'padded': 44582,\n",
       " 'ash': 1077,\n",
       " 'nont': 45930,\n",
       " 'Adds': 46245,\n",
       " 'Sexy': 49131,\n",
       " '733': 49995,\n",
       " 'behavi': 6598,\n",
       " 'fold': 11379,\n",
       " 'cfg': 37581,\n",
       " '1993': 9656,\n",
       " 'Ludwig': 44476,\n",
       " 'rius': 48969,\n",
       " 'neural': 17019,\n",
       " 'orphans': 50213,\n",
       " 'gazed': 50255,\n",
       " '433': 47407,\n",
       " '&': 1222,\n",
       " 'bar': 5657,\n",
       " 'Steal': 47242,\n",
       " 'partial': 13027,\n",
       " 'Power': 4333,\n",
       " 'Chief': 23675,\n",
       " 'unrecogn': 43483,\n",
       " 'expenses': 9307,\n",
       " 'seeming': 31792,\n",
       " 'worsh': 26511,\n",
       " 'penetrate': 28302,\n",
       " 'Thur': 36975,\n",
       " 'ortmund': 34876,\n",
       " 'counter': 24588,\n",
       " 'attrition': 48981,\n",
       " 'Acad': 7116,\n",
       " 'Never': 7236,\n",
       " 'Mercury': 21673,\n",
       " 'bec': 9423,\n",
       " 'acies': 13433,\n",
       " 'lists': 20713,\n",
       " 'Ain': 31899,\n",
       " 'moderate': 47189,\n",
       " 'exploitation': 17238,\n",
       " 'awaited': 39576,\n",
       " 'owers': 3618,\n",
       " 'Newark': 30970,\n",
       " 'demonstr': 4110,\n",
       " 'quality': 13237,\n",
       " 'Patriots': 13104,\n",
       " 'Authority': 11416,\n",
       " 'paying': 32629,\n",
       " 'masturb': 22938,\n",
       " 'Of': 3226,\n",
       " 'Georgetown': 31393,\n",
       " 'outh': 1536,\n",
       " 'rieving': 37418,\n",
       " 'Ragnarok': 40451,\n",
       " 'Racial': 42318,\n",
       " 'learned': 4499,\n",
       " '\\\\/\\\\/': 45422,\n",
       " 'tics': 14094,\n",
       " 'Luke': 11336,\n",
       " 'record': 1700,\n",
       " 'ogly': 34619,\n",
       " 'tumble': 47978,\n",
       " 'ac': 330,\n",
       " 'ATIONS': 18421,\n",
       " 'beverages': 24173,\n",
       " 'formulas': 32126,\n",
       " 'tcp': 48265,\n",
       " 'Concern': 32265,\n",
       " 'abbrevi': 37640,\n",
       " '578': 38907,\n",
       " 'anyways': 32845,\n",
       " 'ettel': 47417,\n",
       " 'coated': 30267,\n",
       " 'reiter': 19291,\n",
       " 'ITAL': 40579,\n",
       " 'Chick': 31939,\n",
       " 'retreat': 13703,\n",
       " 'Lone': 23405,\n",
       " 'volent': 29078,\n",
       " 'ESC': 40251,\n",
       " 'NOT': 11929,\n",
       " 'Edward': 43982,\n",
       " 'pillars': 30485,\n",
       " './': 19571,\n",
       " 'uncertain': 8627,\n",
       " 'posture': 24521,\n",
       " 'Ab': 2275,\n",
       " 'Clinton': 16549,\n",
       " 'Sl': 3454,\n",
       " 'Interest': 12033,\n",
       " 'dial': 38969,\n",
       " 'grips': 31323,\n",
       " 'Monday': 23810,\n",
       " 'intr': 9913,\n",
       " 'importing': 33332,\n",
       " 'town': 3240,\n",
       " 'hyp': 36362,\n",
       " 'bro': 1379,\n",
       " 'cul': 10845,\n",
       " 'guideline': 40888,\n",
       " 'ovich': 18198,\n",
       " 'Micha': 38844,\n",
       " 'snag': 48456,\n",
       " 'Khe': 48888,\n",
       " 'nurse': 15849,\n",
       " 'acan': 50195,\n",
       " 'successor': 17270,\n",
       " ']]': 11907,\n",
       " 'usalem': 10555,\n",
       " 'shape': 5485,\n",
       " 'ambassador': 14791,\n",
       " 'Lean': 45661,\n",
       " 'lab': 2248,\n",
       " 'animal': 41607,\n",
       " 'azz': 8101,\n",
       " 'played': 2826,\n",
       " 'Lear': 14961,\n",
       " 'Jab': 24404,\n",
       " 'Lists': 44968,\n",
       " 'Pierre': 36910,\n",
       " 'scar': 13034,\n",
       " 'Defense': 27300,\n",
       " 'euphem': 48732,\n",
       " 'notable': 12411,\n",
       " 'deal': 1730,\n",
       " 'hog': 40476,\n",
       " 'dimensions': 15225,\n",
       " 'agile': 36710,\n",
       " 'repl': 2186,\n",
       " 'per': 583,\n",
       " 'Jing': 42279,\n",
       " 'observ': 3799,\n",
       " 'invest': 24859,\n",
       " 'perfect': 2818,\n",
       " 'lez': 36858,\n",
       " 'licences': 45475,\n",
       " '275': 23195,\n",
       " 'baum': 24738,\n",
       " '?),': 33924,\n",
       " 'containment': 37149,\n",
       " 'andr': 46273,\n",
       " 'interviewed': 12299,\n",
       " 'pered': 13653,\n",
       " 'Cogn': 26543,\n",
       " 'Footnote': 33795,\n",
       " 'ss': 824,\n",
       " 'soften': 39536,\n",
       " 'icons': 17149,\n",
       " 'needed': 27938,\n",
       " 'counted': 14789,\n",
       " 'repercussions': 34056,\n",
       " '7601': 42752,\n",
       " 'Column': 29201,\n",
       " 'protestors': 36915,\n",
       " 'MAN': 17254,\n",
       " 'aggregation': 46500,\n",
       " 'enraged': 37530,\n",
       " 'Library': 23377,\n",
       " 'Rugby': 26244,\n",
       " 'itivity': 11365,\n",
       " 'verbally': 38119,\n",
       " '': 25992,\n",
       " 'Textures': 39860,\n",
       " 'roximately': 24378,\n",
       " 'condem': 8346,\n",
       " 'maps': 31803,\n",
       " 'redd': 14688,\n",
       " 'collect': 33327,\n",
       " 'blasting': 36998,\n",
       " 'incapac': 38533,\n",
       " 'intermediary': 45193,\n",
       " 'Phil': 4543,\n",
       " 'lations': 49905,\n",
       " 'his': 465,\n",
       " 'express': 4911,\n",
       " 'arguments': 7159,\n",
       " 'boat': 8848,\n",
       " 'Class': 9487,\n",
       " '1983': 13540,\n",
       " 'recorded': 6264,\n",
       " 'rat': 10366,\n",
       " 'Volt': 22702,\n",
       " 'Hogwarts': 30922,\n",
       " 'trenches': 40068,\n",
       " 'vowel': 48617,\n",
       " 'iencies': 22139,\n",
       " 'screenshots': 23322,\n",
       " 'Tables': 33220,\n",
       " '162': 25061,\n",
       " 'garments': 36097,\n",
       " 'util': 22602,\n",
       " 'acia': 47431,\n",
       " 'ODUCT': 28644,\n",
       " 'Thro': 47854,\n",
       " 'classy': 48486,\n",
       " 'Brett': 18726,\n",
       " 'ri': 380,\n",
       " 'reader': 46862,\n",
       " 'Shine': 41249,\n",
       " 'arts': 5889,\n",
       " 'arantine': 37996,\n",
       " 'instinct': 13311,\n",
       " 'ins': 1035,\n",
       " 'Costa': 18133,\n",
       " 'slap': 23905,\n",
       " 'hetto': 35619,\n",
       " 'selectively': 39119,\n",
       " '820': 41739,\n",
       " 'CAN': 44565,\n",
       " 'Earn': 49725,\n",
       " 'interns': 47266,\n",
       " 'grocery': 16918,\n",
       " 'cence': 43696,\n",
       " 'cano': 35490,\n",
       " 'RIPT': 46023,\n",
       " 'Ge': 10082,\n",
       " 'Soviets': 31062,\n",
       " 'ripping': 34759,\n",
       " 'Gro': 42921,\n",
       " 'deliberate': 18988,\n",
       " 'Mayhem': 35450,\n",
       " 'checking': 41004,\n",
       " 'embarked': 36385,\n",
       " 'IPO': 41805,\n",
       " 'Bite': 44540,\n",
       " 'issa': 13808,\n",
       " 'Reach': 25146,\n",
       " 'quirky': 37276,\n",
       " 'Wow': 22017,\n",
       " 'Mo': 16632,\n",
       " 'gz': 34586,\n",
       " '248': 32996,\n",
       " 'BER': 13246,\n",
       " 'brand': 17938,\n",
       " 'handing': 22786,\n",
       " 'groundwork': 40641,\n",
       " 'CAR': 20034,\n",
       " 'listed': 5610,\n",
       " 'Eastern': 8345,\n",
       " 'Yar': 45184,\n",
       " 'Employ': 29733,\n",
       " 'Hal': 40202,\n",
       " 'characteristic': 16704,\n",
       " 'ras': 8847,\n",
       " 'iCloud': 42076,\n",
       " 'Sr': 21714,\n",
       " 'definitely': 4753,\n",
       " 'unauthorized': 22959,\n",
       " 'Translation': 33322,\n",
       " 'modify': 13096,\n",
       " 'relationship': 2776,\n",
       " 'Project': 16775,\n",
       " 'sprang': 43908,\n",
       " 'ians': 1547,\n",
       " 'picnic': 35715,\n",
       " 'were': 22474,\n",
       " 'Brian': 24761,\n",
       " 'Tro': 44095,\n",
       " 'Twitch': 23835,\n",
       " '753': 44550,\n",
       " 'Rebecca': 23489,\n",
       " 'Sounds': 40825,\n",
       " 'haus': 30404,\n",
       " 'Bob': 18861,\n",
       " 'pad': 15636,\n",
       " 'ique': 2350,\n",
       " 'tick': 42298,\n",
       " 'discussion': 5114,\n",
       " 'Texas': 21607,\n",
       " 'krit': 44531,\n",
       " 'auto': 8295,\n",
       " 'Kenny': 22102,\n",
       " 'axter': 40864,\n",
       " 'Osaka': 42429,\n",
       " 'pile': 14540,\n",
       " 'kick': 4829,\n",
       " 'GT': 7963,\n",
       " 'version': 2196,\n",
       " 'okemon': 12717,\n",
       " 'mitted': 3291,\n",
       " 'Tri': 14824,\n",
       " 'oro': 16522,\n",
       " '': 19526,\n",
       " 'dds': 33714,\n",
       " 'O': 46,\n",
       " 'RM': 29820,\n",
       " 'hurdle': 36633,\n",
       " 'pt': 457,\n",
       " 'von': 18042,\n",
       " 'Fab': 14236,\n",
       " 'lect': 801,\n",
       " 'Luxem': 29017,\n",
       " 'ythm': 34853,\n",
       " 'Hits': 28626,\n",
       " 'Us': 4021,\n",
       " '##': 22492,\n",
       " 'dean': 34798,\n",
       " 'ako': 25496,\n",
       " 'RH': 35662,\n",
       " 'Romance': 36555,\n",
       " 'Stout': 40275,\n",
       " 'illac': 40607,\n",
       " 'smarter': 23714,\n",
       " 'fisheries': 41440,\n",
       " 'Mov': 44795,\n",
       " 'LAT': 42355,\n",
       " 'protested': 27278,\n",
       " 'eneg': 46495,\n",
       " 'Bot': 20630,\n",
       " 'Nanto': 47614,\n",
       " 'Photos': 9434,\n",
       " 'unpublished': 42686,\n",
       " 'Mage': 17323,\n",
       " 'funer': 49831,\n",
       " 'submerged': 37930,\n",
       " 'stag': 35251,\n",
       " 'Imam': 38386,\n",
       " 'Asian': 7740,\n",
       " 'Caption': 11260,\n",
       " 'Ign': 16583,\n",
       " 'Darren': 26203,\n",
       " 'Dumbledore': 27442,\n",
       " 'adhesive': 43608,\n",
       " 'rup': 12618,\n",
       " 'Wing': 13405,\n",
       " 'forcefully': 35701,\n",
       " 'Macedonia': 36353,\n",
       " 'elections': 7024,\n",
       " 'daring': 27939,\n",
       " 'phen': 31024,\n",
       " 'phalt': 41942,\n",
       " 'itself': 2346,\n",
       " 'bage': 13866,\n",
       " 'OUN': 19385,\n",
       " '218': 28727,\n",
       " 'sailor': 43272,\n",
       " 'separating': 27259,\n",
       " 'cru': 32838,\n",
       " 'texture': 41293,\n",
       " 'MSM': 44401,\n",
       " 'Scotch': 46755,\n",
       " 'ments': 902,\n",
       " 'pies': 47199,\n",
       " 'deserves': 14071,\n",
       " 'habitual': 46020,\n",
       " 'Lie': 47918,\n",
       " 'Expect': 23600,\n",
       " 'Crimean': 48062,\n",
       " 'teaches': 17324,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 has a vocab of 50257 words, the provided vocab has 20500 words, and there are 3871 words in their intersection.\n"
     ]
    }
   ],
   "source": [
    "gpt_vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "print('GPT2 has a vocab of %d words, the provided vocab has %d words, and there are %d words in their intersection.' \n",
    "      %(len(gpt_vocab), len(vocab), len(set(vocab).intersection(gpt_vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Increase the vocabulary\\n\\nnum_added_toks = tokenizer.add_tokens(vocab_with_symbol)\\n\\nprint('We have added', num_added_toks, 'tokens. New vocab size: ', len(set(tokenizer.get_vocab().keys())))\\n\\n# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\\n\\nmodel.resize_token_embeddings(len(tokenizer))\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Increase the vocabulary\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(vocab_with_symbol)\n",
    "\n",
    "print('We have added', num_added_toks, 'tokens. New vocab size: ', len(set(tokenizer.get_vocab().keys())))\n",
    "\n",
    "# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dp', 'kg', 'set', 'prog', 'name']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('dpkg set prog name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre process training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9d82746b4bd06c73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\SP003DA2\\.cache\\huggingface\\datasets\\csv\\default-9d82746b4bd06c73\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95af0c88ad74cd9ae4016f53c89e715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c8c28a3ff94e359978b768b16ed18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\SP003DA2\\.cache\\huggingface\\datasets\\csv\\default-9d82746b4bd06c73\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf612734bb402694d542b71199d040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load names in a transformer format\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "dataset_dict = load_dataset('csv', data_files='names.csv')\n",
    "dataset = dataset_dict['train']\n",
    "dataset = dataset.train_test_split(test_size=0.05, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Names'],\n",
       "        num_rows: 281906\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Names'],\n",
       "        num_rows: 14838\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Names': ' secs \\n'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2d801c485e4b70a90cf925e9fb9c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832ebafc611e468e86f7739f0c9c6c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize inputs\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"Names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5412, 900, 3298, 13536, 220, 198],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' handle set global enables \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[\"train\"][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attribute'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(11688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate inputs\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Drop the small remainder\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00e51a2416a4507a2c16c65b2d5cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/282 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718c193bf1514caa96ca9b42152289e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "#    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 56369\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test incremental no break \\n rl reap \\n check name \\n login manager skeleton get controllers \\n mcd null able variant equal \\n put h'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inputs changed:\n",
    "tokenizer.decode(lm_dataset[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and fine-tune GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(lr=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "train_set = lm_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_set = lm_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3524/3524 [==============================] - 13831s 4s/step - loss: 4.3229 - val_loss: 3.9821\n",
      "Epoch 2/3\n",
      "3524/3524 [==============================] - 13900s 4s/step - loss: 3.9294 - val_loss: 3.7956\n",
      "Epoch 3/3\n",
      "3524/3524 [==============================] - 13650s 4s/step - loss: 3.7740 - val_loss: 3.6951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a112480808>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-wikitext2\"\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./clm_model_save/logs\")\n",
    "\n",
    "#push_to_hub_callback = PushToHubCallback(\n",
    "#    output_dir=\"./clm_model_save\",\n",
    "#    tokenizer=tokenizer,\n",
    "#    hub_model_id=push_to_hub_model_id,\n",
    "#)\n",
    "\n",
    "callbacks = [tensorboard_callback]#, push_to_hub_callback]\n",
    "\n",
    "model.fit(train_set, validation_data=test_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('FineTuned_GPT2_TF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-78-7146d0bffb59>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-78-7146d0bffb59>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    export OPENAI_GPT2_CHECKPOINT_PATH='FineTuned_GPT2_TF'\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export OPENAI_GPT2_CHECKPOINT_PATH='FineTuned_GPT2_TF'\n",
    "\n",
    "transformers-cli convert --model_type gpt2 --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH --pytorch_dump_output $PYTORCH_DUMP_OUTPUT [--config OPENAI_GPT2_CONFIG] [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./FineTuned_GPT2_TF\", from_tf=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check here:\n",
    "# https://discuss.huggingface.co/t/prohibit-gpt-2-from-generating-some-words-on-a-condition/4823\n",
    "# https://discuss.huggingface.co/t/example-of-prefix-allowed-tokens-fn-while-text-generation/6635\n",
    "# https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "# and look for: prefix_allowed_tokens_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dp', 'kg', 'set', 'prog', 'name']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('dpkg set prog name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated:  \n",
      " caml lwt unix get type \n",
      " caml lwt unix get\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate()  # do greedy decoding\n",
    "print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0: The dog \n",
      " gtk file chooser get type \n",
      " org gnome session manager call register client sync \n",
      " bamf match er get extension \n",
      " gf dbus screenshot proxy new\n",
      "Generated 1: The dog \n",
      " on read \n",
      " add to list \n",
      " snd mixer elem get volume \n",
      " caml oasis license find \n",
      " test initialize stress cb \n",
      " get value \n",
      "Generated 2: The dog \n",
      " read file \n",
      " gsm exported client skeleton finalize \n",
      " caml unit logger \n",
      " str ncpy \n",
      " cb get property \n",
      " uwf to ple\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The dog\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True\n",
    ")\n",
    "\n",
    "for i in range(3):  #  3 output sequences were generated\n",
    "    print(f\"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0: dpkg set prog name \n",
      " xfpm power management call get idle time \n",
      " caml\n",
      "Generated 1: dpkg set prog name \n",
      " xdp impl account skeleton handle set property \n",
      " cdw cd\n",
      "Generated 2: dpkg set prog name \n",
      " xdp impl account skeleton get version \n",
      " caml exp lib\n"
     ]
    }
   ],
   "source": [
    "prompt = 'dpkg set prog name'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, temperature=0.7, num_return_sequences=3, do_sample=True\n",
    ")\n",
    "\n",
    "for i in range(3):  #  3 output sequences were generated\n",
    "    print(f\"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 0: dpkg set prog name \n",
      " games plugin page constructor \n",
      " ntfs fuse open dir \n",
      "Generated 1: dpkg set prog name \n",
      " do move \n",
      " caml oasis features make fun \n",
      "\n",
      "Generated 2: dpkg set prog name \n",
      " dl get \n",
      " lzma decode \n",
      " wix\n"
     ]
    }
   ],
   "source": [
    "prompt = 'dpkg set prog name'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "\n",
    "force_words = 'functional'\n",
    "force_words_ids = tokenizer.encode(force_words, return_tensors=\"tf\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    temperature=0.7, \n",
    "    num_return_sequences=3,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "for i in range(3):  #  3 output sequences were generated\n",
    "    print(f\"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model.generate(input_ids, do_sample=True, max_length=len(input_ids[0])+1, num_return_sequences=2, output_scores=True)\n",
    "#tokenizer.batch_decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
